{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      ****     SENTIMENT ANALYSIS OF TWEETS       ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud \n",
    "from matplotlib import pyplot as plt\n",
    "from porter_stemmer import PorterStemmer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                              tweet\n",
      "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1          0  is upset that he can't update his Facebook by ...\n",
      "2          0  @Kenichan I dived many times for the ball. Man...\n",
      "3          0    my whole body feels itchy and like its on fire \n",
      "4          0  @nationwideclass no, it's not behaving at all....\n",
      "5          0                      @Kwesidei not the whole crew \n",
      "6          0                                        Need a hug \n",
      "7          0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "8          0               @Tatiana_K nope they didn't have it \n",
      "9          0                          @twittera que me muera ? \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input_data/tweet_data.csv\",names = [\"sentiment\",\"date\",\"user\",\"xgfg\",\"dxgfd\",\"tweet\"] , encoding='latin-1')\n",
    "data = data.drop(columns=[\"date\",\"user\",\"xgfg\",\"dxgfd\"])\n",
    "data.head(10)\n",
    "print (data.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    \n",
    "    #remove @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    \n",
    "#     # Remove tickers\n",
    "#     tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # To lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "#     tweet = re.sub(r'#\\w*', '', tweet)\n",
    "\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "    \n",
    "    # Remove words with 2 or fewer letters\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "    \n",
    "    # Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    tweet = re.sub(' +', ' ',tweet)\n",
    "    \n",
    "    # Remove single space remaining at the front of the tweet.\n",
    "    tweet = tweet.lstrip(' ')  \n",
    "    \n",
    "    # Removing Stopwords from tweet using sklearn.feature_extraction\n",
    "    split_list = tweet.split(\" \")\n",
    "    tweet = [ word for word in split_list if word not in stop_words.ENGLISH_STOP_WORDS ]\n",
    "    \n",
    "    # Stemming the \n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ ps.stem(word) for word in tweet ]\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "processed_data = list()\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    processed_data.append(processTweet(row['tweet']))\n",
    "    \n",
    "# data['processed'] = processed_data\n",
    "# data.head()                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww bummer shoulda got david carr dai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset updat facebook tex result school todai b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>dive time ball manag save rest bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>bodi feel itchi like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>behav mad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet  \\\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          0  is upset that he can't update his Facebook by ...   \n",
       "2          0  @Kenichan I dived many times for the ball. Man...   \n",
       "3          0    my whole body feels itchy and like its on fire    \n",
       "4          0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                           processed  \n",
       "0            awww bummer shoulda got david carr dai   \n",
       "1  upset updat facebook tex result school todai b...  \n",
       "2               dive time ball manag save rest bound  \n",
       "3                              bodi feel itchi like   \n",
       "4                                         behav mad   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['processed'] = processed_data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['processed']\n",
    "Y = data['sentiment']\n",
    "X_train_val, X_test , Y_train_val, Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val,Y_train_val,test_size=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Files For Train,Test and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, Y_train],axis='columns').reset_index(drop=True)\n",
    "train_df.to_csv(\"./input_data/train_df.csv\", sep=',')         \n",
    "\n",
    "validation_df = pd.concat([X_val, Y_val],axis='columns').reset_index(drop=True)\n",
    "validation_df.to_csv(\"./input_data/validation_df.csv\", sep=',')\n",
    "\n",
    "test_df = pd.concat([X_test, Y_test],axis='columns').reset_index(drop=True)\n",
    "test_df.to_csv(\"./input_data/test_df.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input_data/train_df.csv\")\n",
    "def wordle(df):\n",
    "    all_words = []\n",
    "    for line in df['processed']:\n",
    "        try:\n",
    "            all_words.extend(line.split())\n",
    "        except:\n",
    "            pass\n",
    "    text = \" \".join(all_words)\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_df= df[df['sentiment']==0].reset_index(drop=True)\n",
    "# positive_df = df[df['sentiment']==4].reset_index(drop=True)\n",
    "# wordle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960000, 178074)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "VC = vectorizer.fit_transform(X_train)\n",
    "print (VC.shape)\n",
    "# print(VC)\n",
    "# print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 174240)\t0.2413243318571412\n",
      "  (0, 24012)\t0.2713369749038635\n",
      "  (0, 148317)\t0.3314144375781771\n",
      "  (0, 172582)\t0.428213932983034\n",
      "  (0, 109052)\t0.23827451727710805\n",
      "  (0, 101795)\t0.29898501681432677\n",
      "  (0, 39542)\t0.3590078921381936\n",
      "  (0, 126320)\t0.25644102567420435\n",
      "  (0, 136091)\t0.30601943843644686\n",
      "  (0, 147882)\t0.3747969613500881\n",
      "  (1, 161954)\t0.2619983181785407\n",
      "  (1, 65268)\t0.22165012590920238\n",
      "  (1, 94096)\t0.1457096369806136\n",
      "  (1, 43605)\t0.3104860137153065\n",
      "  (1, 154133)\t0.21184429953581957\n",
      "  (1, 41927)\t0.13693288774513504\n",
      "  (1, 166630)\t0.18737714388638274\n",
      "  (1, 117198)\t0.35572265136219866\n",
      "  (1, 39734)\t0.36059998952394784\n",
      "  (1, 67388)\t0.5105180136222788\n",
      "  (1, 122654)\t0.2975258657128238\n",
      "  (1, 130683)\t0.2452247596661462\n",
      "  (2, 130683)\t0.36836743657028304\n",
      "  (2, 152849)\t0.23533125268915317\n",
      "  (2, 67954)\t0.2960518270914764\n",
      "  :\t:\n",
      "  (959996, 23330)\t0.2765055267163925\n",
      "  (959996, 165006)\t0.287655600890024\n",
      "  (959996, 65068)\t0.4262177054680919\n",
      "  (959997, 156217)\t0.19266956568546492\n",
      "  (959997, 65198)\t0.1595053678701726\n",
      "  (959997, 49951)\t0.21620509403350838\n",
      "  (959997, 48376)\t0.275762130826542\n",
      "  (959997, 109328)\t0.3139168950756304\n",
      "  (959997, 45398)\t0.3101714797870688\n",
      "  (959997, 50759)\t0.3049559176192026\n",
      "  (959997, 39667)\t0.3232947235863198\n",
      "  (959997, 118262)\t0.37639832606035073\n",
      "  (959997, 35050)\t0.5298147099290081\n",
      "  (959998, 34787)\t0.5486087024858428\n",
      "  (959998, 132689)\t0.34826810394657676\n",
      "  (959998, 14305)\t0.49465955370801906\n",
      "  (959998, 96504)\t0.32310504659909856\n",
      "  (959998, 113529)\t0.4781766139386514\n",
      "  (959999, 41927)\t0.23749779912198163\n",
      "  (959999, 140794)\t0.31043664192328596\n",
      "  (959999, 38574)\t0.44847070132981137\n",
      "  (959999, 74658)\t0.3376150037854241\n",
      "  (959999, 36005)\t0.4085169808940141\n",
      "  (959999, 49951)\t0.3613306508837794\n",
      "  (959999, 167237)\t0.4844254979710168\n"
     ]
    }
   ],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# features = tfidf.fit_transform(X_train)\n",
    "# print (features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cf6734b2b9af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ])\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[1;32m    192\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    515\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                                       accept_large_sparse=accept_large_sparse)\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    319\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipeline = Pipeline([\n",
    "  ('counts', CountVectorizer()),\n",
    "  ('tf_idf', TfidfTransformer()),\n",
    "  ('classifier', GaussianNB())\n",
    "])\n",
    "pipeline.fit(np.array(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counts', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        str...e, sublinear_tf=False, use_idf=True)), ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))])\n"
     ]
    }
   ],
   "source": [
    "print (pipeline)\n",
    "# gnb = GaussianNB()\n",
    "# model_nb = gnb.fit(VC.todense(), Y_train)\n",
    "# Y_pred = model_nb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(str):\n",
    "#     return str.split(\" \")\n",
    "\n",
    "# def stemming(list_of_tokens):\n",
    "#     ps = PorterStemmer()\n",
    "#     return [ ps.stem(word) for word in list_of_tokens ] \n",
    "\n",
    "# word_list = []\n",
    "# for st in data.iloc[:10,1]:\n",
    "#     tokens = tokenize(st)\n",
    "#     stemmed = stemming(tokens)\n",
    "#     for wd in stemmed:\n",
    "#         if wd in ENGLISH_STOP_WORDS:\n",
    "# #             print wd\n",
    "#             stemmed.remove(wd)\n",
    "#         if wd == '':\n",
    "#             stemmed.remove(wd)\n",
    "#     word_list += [stemmed]\n",
    "    \n",
    "# print word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_pattern(input_txt, pattern): \n",
    "#     r = re.findall(pattern, input_txt)\n",
    "#     for i in r:\n",
    "#         input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "#     return input_txt\n",
    "\n",
    "# # remove twitter handles (@user)\n",
    "# data.iloc[:10,1] = np.vectorize(remove_pattern)(data.iloc[:10,1], \"@[\\w]*\")\n",
    "# print (data.iloc[:10,1])\n",
    "\n",
    "# # # remove url\n",
    "# # data.iloc[:10,1] = np.vectorize(remove_pattern)(data.iloc[:10,1],)# \"^https?:\\/\\/.*[\\r\\n]*\")#, text, flags=re.MULTILINE))\n",
    "# # print (data.iloc[:10,1])\n",
    "\n",
    "# # remove special characters, numbers, punctuations\n",
    "# data.iloc[:10,1] = data.iloc[:10,1].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# print (data.iloc[:10,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
